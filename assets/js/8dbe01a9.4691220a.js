"use strict";(self.webpackChunkseamless_cicd_github_io=self.webpackChunkseamless_cicd_github_io||[]).push([[120],{3905:(e,t,i)=>{i.d(t,{Zo:()=>d,kt:()=>m});var n=i(7294);function a(e,t,i){return t in e?Object.defineProperty(e,t,{value:i,enumerable:!0,configurable:!0,writable:!0}):e[t]=i,e}function s(e,t){var i=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),i.push.apply(i,n)}return i}function o(e){for(var t=1;t<arguments.length;t++){var i=null!=arguments[t]?arguments[t]:{};t%2?s(Object(i),!0).forEach((function(t){a(e,t,i[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(i)):s(Object(i)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(i,t))}))}return e}function r(e,t){if(null==e)return{};var i,n,a=function(e,t){if(null==e)return{};var i,n,a={},s=Object.keys(e);for(n=0;n<s.length;n++)i=s[n],t.indexOf(i)>=0||(a[i]=e[i]);return a}(e,t);if(Object.getOwnPropertySymbols){var s=Object.getOwnPropertySymbols(e);for(n=0;n<s.length;n++)i=s[n],t.indexOf(i)>=0||Object.prototype.propertyIsEnumerable.call(e,i)&&(a[i]=e[i])}return a}var l=n.createContext({}),c=function(e){var t=n.useContext(l),i=t;return e&&(i="function"==typeof e?e(t):o(o({},t),e)),i},d=function(e){var t=c(e.components);return n.createElement(l.Provider,{value:t},e.children)},u="mdxType",p={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},h=n.forwardRef((function(e,t){var i=e.components,a=e.mdxType,s=e.originalType,l=e.parentName,d=r(e,["components","mdxType","originalType","parentName"]),u=c(i),h=a,m=u["".concat(l,".").concat(h)]||u[h]||p[h]||s;return i?n.createElement(m,o(o({ref:t},d),{},{components:i})):n.createElement(m,o({ref:t},d))}));function m(e,t){var i=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var s=i.length,o=new Array(s);o[0]=h;var r={};for(var l in t)hasOwnProperty.call(t,l)&&(r[l]=t[l]);r.originalType=e,r[u]="string"==typeof e?e:a,o[1]=r;for(var c=2;c<s;c++)o[c]=i[c];return n.createElement.apply(null,o)}return n.createElement.apply(null,i)}h.displayName="MDXCreateElement"},5906:(e,t,i)=>{i.r(t),i.d(t,{assets:()=>l,contentTitle:()=>o,default:()=>p,frontMatter:()=>s,metadata:()=>r,toc:()=>c});var n=i(7462),a=(i(7294),i(3905));const s={title:"Case Study"},o="Case Study",r={unversionedId:"case-study",id:"case-study",title:"Case Study",description:"1. Introduction",source:"@site/docs/case-study.md",sourceDirName:".",slug:"/case-study",permalink:"/case-study",draft:!1,tags:[],version:"current",frontMatter:{title:"Case Study"}},l={},c=[{value:"1. Introduction",id:"1-introduction",level:2},{value:"2. Evolution of Deployment Processes",id:"2-evolution-of-deployment-processes",level:2},{value:"2.1 Version Control Systems",id:"21-version-control-systems",level:3},{value:"2.2 Manual Deployments are Slow and Unreliable",id:"22-manual-deployments-are-slow-and-unreliable",level:3},{value:"2.3 Automated Deployments Improve Speed and Reliability",id:"23-automated-deployments-improve-speed-and-reliability",level:3},{value:"3. CI/CD Pipelines",id:"3-cicd-pipelines",level:2},{value:"3.1 Stages of a CI/CD Pipeline",id:"31-stages-of-a-cicd-pipeline",level:3},{value:"3.2 Continuous Integration, Delivery, and Deployment",id:"32-continuous-integration-delivery-and-deployment",level:3},{value:"4. Balancing Safety and Velocity",id:"4-balancing-safety-and-velocity",level:2},{value:"5. CI/CD for Monoliths and Microservices",id:"5-cicd-for-monoliths-and-microservices",level:2},{value:"5.1 Different Communication Methods",id:"51-different-communication-methods",level:3},{value:"5.2 Different Deployment Methods",id:"52-different-deployment-methods",level:3},{value:"6. CI/CD Challenges with Microservices",id:"6-cicd-challenges-with-microservices",level:2},{value:"6.1 Pipeline Management Challenges",id:"61-pipeline-management-challenges",level:3},{value:"The Many-Pipeline Problem",id:"the-many-pipeline-problem",level:4},{value:"The Shared Segment Solution",id:"the-shared-segment-solution",level:4},{value:"The Single, Parameterized Pipeline Solution",id:"the-single-parameterized-pipeline-solution",level:4},{value:"6.2 Microservice Testing Challenges",id:"62-microservice-testing-challenges",level:3},{value:"Solutions for Testing Service Interactions",id:"solutions-for-testing-service-interactions",level:4},{value:"7. Manually Building a CI/CD Pipeline for Microservices",id:"7-manually-building-a-cicd-pipeline-for-microservices",level:2},{value:"8. Existing Solutions",id:"8-existing-solutions",level:2},{value:"8.1 DIY Solutions",id:"81-diy-solutions",level:3},{value:"8.2 Commercial Solutions",id:"82-commercial-solutions",level:3},{value:"8.3 A Solution for Our Use Case",id:"83-a-solution-for-our-use-case",level:3},{value:"9. Introducing Seamless",id:"9-introducing-seamless",level:2},{value:"9.1 Setting Up Seamless",id:"91-setting-up-seamless",level:3},{value:"9.2 Using Seamless",id:"92-using-seamless",level:3},{value:"Connecting Services to the Pipeline",id:"connecting-services-to-the-pipeline",level:3},{value:"Running the Pipeline",id:"running-the-pipeline",level:3},{value:"Monitoring the Pipeline",id:"monitoring-the-pipeline",level:3},{value:"10. Seamless\u2019s Architecture",id:"10-seamlesss-architecture",level:2},{value:"11. Building the Core Pipeline Functionality",id:"11-building-the-core-pipeline-functionality",level:2},{value:"11.1 Modeling and Storing Data",id:"111-modeling-and-storing-data",level:3},{value:"11.2 Managing Pipeline Execution",id:"112-managing-pipeline-execution",level:3},{value:"11.3 Running Tasks",id:"113-running-tasks",level:3},{value:"11.4 Automating Pipeline Runs",id:"114-automating-pipeline-runs",level:3},{value:"12. Improving Core Functionality",id:"12-improving-core-functionality",level:2},{value:"12.1 Sharing Data Among Containers",id:"121-sharing-data-among-containers",level:3},{value:"12.2 Running Docker Commands Inside Docker Containers",id:"122-running-docker-commands-inside-docker-containers",level:3},{value:"12.3 Integration Testing",id:"123-integration-testing",level:3},{value:"12.4 Manual Approval of Staging Environments",id:"124-manual-approval-of-staging-environments",level:3},{value:"12.5 Rollbacks",id:"125-rollbacks",level:3},{value:"12.6 Automatic Deployment of Fargate Clusters",id:"126-automatic-deployment-of-fargate-clusters",level:3},{value:"13. Beyond the Core Pipeline",id:"13-beyond-the-core-pipeline",level:2},{value:"13.1 Designing for Performance and Scale",id:"131-designing-for-performance-and-scale",level:3},{value:"Serverless Backend",id:"serverless-backend",level:4},{value:"Parallel Execution of State Machines",id:"parallel-execution-of-state-machines",level:4},{value:"13.2 Notifications",id:"132-notifications",level:3},{value:"13.3 Logging",id:"133-logging",level:3},{value:"13.4 Basic Security",id:"134-basic-security",level:3},{value:"OAuth",id:"oauth",level:4},{value:"Private Subnets",id:"private-subnets",level:4},{value:"14. Future Work",id:"14-future-work",level:2},{value:"References",id:"references",level:2}],d={toc:c},u="wrapper";function p(e){let{components:t,...s}=e;return(0,a.kt)(u,(0,n.Z)({},d,s,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"case-study"},"Case Study"),(0,a.kt)("h2",{id:"1-introduction"},"1. Introduction"),(0,a.kt)("p",null,"Software engineering teams strive to integrate and deliver code changes quickly while keeping code high-quality and bug-free. To achieve this, many members of the software engineering community have embraced a philosophy and practice called CI/CD (continuous integration and delivery). CI/CD automates processes such as building and testing code, merging it into a shared repository, packaging it into a deployable artifact, and delivering it to production. As the application moves through these phases, confidence that it can safely be released into production grows."),(0,a.kt)("p",null,"Microservice architectures, however, present unique testing and deployment challenges. Unlike a single application with a single deployment pipeline, microservice architectures use individual deployment pipelines for individual services. Additionally, microservices must function properly within the context of the entire system before they are deployed to production. Adequate testing must be implemented to address these complexities."),(0,a.kt)("p",null,"This case study describes how we built ",(0,a.kt)("strong",{parentName:"p"},"Seamless"),", a self-hosted, open-source, cloud-native CI/CD solution tailored for microservices. Seamless offers a low-configuration platform for automating the testing, building, and deployment of containerized microservice applications."),(0,a.kt)("h2",{id:"2-evolution-of-deployment-processes"},"2. Evolution of Deployment Processes"),(0,a.kt)("p",null,"A deployment process is a sequence of steps for building, testing, and deploying software. Before running the deployment process, it is crucial to identify the source of the code that will be used. This is where version control systems come into play."),(0,a.kt)("h3",{id:"21-version-control-systems"},"2.1 Version Control Systems"),(0,a.kt)("p",null,"Version control systems such as Git enable developers to track changes, collaborate, and revert to previous versions of code.",(0,a.kt)("sup",{parentName:"p",id:"fnref-1"},(0,a.kt)("a",{parentName:"sup",href:"#fn-1",className:"footnote-ref"},"1"))," By creating branches, developers can isolate changes and work on new features independently, which are eventually merged into a central branch, commonly referred to as the main or master branch. This branch's code will be deployed to production, and this is where the deployment process kicks in."),(0,a.kt)("p",null,"While most deployment processes utilize version control systems, the path from version control to deployment can vary significantly, and it can be either manual or automatic."),(0,a.kt)("h3",{id:"22-manual-deployments-are-slow-and-unreliable"},"2.2 Manual Deployments are Slow and Unreliable"),(0,a.kt)("p",null,"Manual deployments are time-consuming. Firstly, there is usually a delay between the deployment request and the start of deployment procedures. The code ready for release remains in the version control system until the person or team responsible for deployment is notified."),(0,a.kt)("p",null,"Once the deployment process kicks off, teams must perform a long series of tasks to bring that code to production. This might include running scripts, adjusting configuration, checking code quality, and monitoring progress. When multiple teams are responsible for different parts of the deployment process, scheduling and coordination issues could cause delays. For example, TrueCar's release cycle involved Change Management tickets, which each team had to file eight days before deployment."),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"Manual deployment delays",src:i(1784).Z,width:"1162",height:"359"})),(0,a.kt)("p",null,"Additionally, manual deployments are error-prone and can lead to unexpected bugs entering production. Humans are bad at performing rote activities, leading to errors when configuring servers, setting up environments, and performing manual tests. To make matters worse, team members often use different operating systems with different environmental configurations. By extension, the team members\u2019 machines might be configured differently from production servers. Even if tests pass locally, they might not pass in production, introducing bugs into the system."),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"Same app different results",src:i(428).Z,width:"700",height:"330"})),(0,a.kt)("p",null,"By defining deployment steps as part of a partially or fully automated pipeline, teams can achieve greater consistency and repeatability, resulting in faster and more reliable deployments."),(0,a.kt)("h3",{id:"23-automated-deployments-improve-speed-and-reliability"},"2.3 Automated Deployments Improve Speed and Reliability"),(0,a.kt)("p",null,"When a deployment process evolves from manual execution of scripts to an automated series of interconnected steps, it becomes known as a ",(0,a.kt)("strong",{parentName:"p"},"deployment pipeline"),". CI/CD (continuous integration and continuous delivery) is a methodology that embraces such automation. CI/CD pipelines provide superior guarantees that the deployment steps will be executed in a consistent and repeatable way, resulting in faster and more reliable deployments."),(0,a.kt)("p",null,"Automated deployments treat version control systems as more than just code storage solutions. Version control systems plug directly into deployment pipelines. Since version control systems are notified when developers make a commit, they can automatically trigger whatever automated system is responsible for deploying the code, which will carry out the deployment process. This eliminates delay between deployment requests and pipeline initiation. This is in stark contrast to manual deployments, where such delays are common."),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"Manual vs automatic deployment",src:i(6197).Z,width:"852",height:"339"})),(0,a.kt)("p",null,"Choosing to ",(0,a.kt)("em",{parentName:"p"},"fully")," automate deployments all the way through production can drastically shorten the time between release cycles, improving the rate at which QA and customers can provide feedback. For example, once TrueCar switched from a waterfall-style approach to a fully automated pipeline, they transitioned from a \u201cburdensome weekly release cycle to deploying code up to 100 times per week\u201d."),(0,a.kt)("p",null,"Automated deployments provide greater reliability by significantly reducing the possibility of human error. An automated pipeline is less prone to mistakes as it executes commands automatically, eliminating the need for human intervention.Quality checks, such as linting and testing, are integrated directly into the pipelines to ensure they are consistently performed with every deployment. Automated testing plays a crucial role in identifying bugs early on, allowing for incremental fixes to be made and preventing issues from accumulating."),(0,a.kt)("h2",{id:"3-cicd-pipelines"},"3. CI/CD Pipelines"),(0,a.kt)("h3",{id:"31-stages-of-a-cicd-pipeline"},"3.1 Stages of a CI/CD Pipeline"),(0,a.kt)("p",null,"A deployment pipeline is essential for delivering code changes from development to production. Although there is no one-size-fits-all pipeline, the steps typically fall under one of the following four stages:"),(0,a.kt)("ol",null,(0,a.kt)("li",{parentName:"ol"},"The ",(0,a.kt)("strong",{parentName:"li"},"Source stage")," connects the pipeline to a repository hosting platform such as GitHub. Specified triggers such as opening a pull request or merging into main will initiate the pipeline."),(0,a.kt)("li",{parentName:"ol"},"The ",(0,a.kt)("strong",{parentName:"li"},"Testing stage")," executes tests against the updated application to ensure code quality and verify that the code functions as expected. Standard forms of testing include static code analysis, unit testing, and integration testing. Static code analysis checks for stylistic issues and basic programmatic vulnerabilities; tools include ESLint and RuboCop. Unit testing verifies the functionality of code components individually; tools include Jest and RSpec. Integration testing confirms proper interactions between application components; tools include Cypress and Selenium."),(0,a.kt)("li",{parentName:"ol"},"The ",(0,a.kt)("strong",{parentName:"li"},"Build stage")," bundles the updated source code with its dependencies and compiles this into a single deployable artifact; tools include Webpack and Docker."),(0,a.kt)("li",{parentName:"ol"},"The ",(0,a.kt)("strong",{parentName:"li"},"Deployment stage")," deploys the built artifact to one or more environments. Typically, this includes a Staging (Pre-Production) environment used by QA teams to review the application and give approval, as well as a Production environment that is accessible to end users and represents the final outcome of the deployment process. Examples of deployment destinations are Amazon Web Services (AWS) Fargate and Google Cloud Run.")),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"Stages",src:i(9339).Z,width:"1559",height:"557"})),(0,a.kt)("p",null,"Most deployments follow these stages, but the degree to which the pipeline is automated depends on the level of adoption of continuous integration, delivery, and deployment."),(0,a.kt)("h3",{id:"32-continuous-integration-delivery-and-deployment"},"3.2 Continuous Integration, Delivery, and Deployment"),(0,a.kt)("p",null,"CI/CD pipelines aim to fulfill three primary objectives, but it is rare for most pipelines to achieve all three completely. These objectives are continuous integration, continuous delivery, and continuous deployment."),(0,a.kt)("p",null,"Continuous integration is the practice of regularly merging code into the main branch of a central repository after passing a series of tests. Continuous delivery extends continuous integration by automatically preparing code changes for release, without necessarily releasing them. Continuous deployment is the hallmark of a well-established CI/CD system: executed builds are immediately released into production."),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"CI/CD",src:i(4437).Z,width:"958",height:"593"})),(0,a.kt)("p",null,"As companies adopt continuous integration, delivery, and deployment practices to different extents, they must consider a new tradeoff: how to balance safety with velocity."),(0,a.kt)("h2",{id:"4-balancing-safety-and-velocity"},"4. Balancing Safety and Velocity"),(0,a.kt)("p",null,"Introducing automation can result in an inevitable tradeoff. While faster code deployment shortens the development lifecycle, it may also increase the likelihood of bugs and errors. Conversely, prioritizing safety by running more tests and validations can reduce velocity."),(0,a.kt)("p",null,"Teams can make a number of CI/CD-related decisions to fine-tune the location of their deployment pipeline on this safety-velocity spectrum."),(0,a.kt)("ol",null,(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("strong",{parentName:"li"},"Branching Strategy:")," Traditional feature branching workflows such as GitHub Flow prioritize safety by reducing the risk of bad code being pushed to main. In contrast, trunk-based development prioritizes speed by encouraging direct commits to main.")),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"Branching strategies",src:i(2708).Z,width:"1269",height:"835"})),(0,a.kt)("ol",{start:2},(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("strong",{parentName:"li"},"Merging Strategy:")," Teams that adopt a feature branching workflow can automatically merge pull requests if tests pass or require a team member to manually perform the merge. While auto-merging can speed up the pipeline, it may lead to merging code that has not been adequately tested and reviewed if the auto-merge criteria are not carefully selected.")),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"Auto merge",src:i(9449).Z,width:"1551",height:"420"})),(0,a.kt)("ol",{start:3},(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("strong",{parentName:"li"},"Continuous Deployment:")," Once code passes tests and is packaged into a build artifact, the pipeline deploys it straight to production without first using a pre-production environment. This enables teams to deliver the product to end users faster.")),(0,a.kt)("p",null,"CI/CD tools can be configured to address the challenge of balancing safety and velocity. However, new challenges arise when CI/CD pipelines are used for different application architectures such as monoliths and microservices."),(0,a.kt)("h2",{id:"5-cicd-for-monoliths-and-microservices"},"5. CI/CD for Monoliths and Microservices"),(0,a.kt)("p",null,"A monolithic application is a single unit containing tightly-coupled components. Its business logic is housed in a single codebase, often a single repository. A microservices architecture consists of independent, loosely coupled services distributed across the network; it may use multiple repositories. Monolithic architecture has historically been the dominant approach to building applications, but this has shifted toward microservices out of a need for more agility and scalability. The differences in these architectures means that they use CI/CD pipelines in different ways, each accompanied by a distinct set of challenges."),(0,a.kt)("h3",{id:"51-different-communication-methods"},"5.1 Different Communication Methods"),(0,a.kt)("p",null,"The way an application\u2019s components communicate determines how tests are performed, which influences how a CI/CD pipeline\u2019s Test stage is configured. All components of a monolith run within the same application, usually in a singular process. The application\u2019s modules communicate via function calls, which are fast and reliable. In contrast, microservices communicate remotely via network calls (e.g. using HTTP), which can be slow and unreliable due to network issues. This means that CI/CD pipelines for microservices must offer more complex functionality to perform inter-service testing over the network."),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"Monolith vs microservices",src:i(1554).Z,width:"1211",height:"395"})),(0,a.kt)("h3",{id:"52-different-deployment-methods"},"5.2 Different Deployment Methods"),(0,a.kt)("p",null,"For monoliths the entire codebase is compiled into a single executable that is deployed to production. In contrast, microservices are deployed as independent units. This is an advantage because small services are easier to update compared to a large monolith. Microservices are smaller and have fewer dependencies than a monolith, which speeds up deployment and reduces time-to-market. Furthermore, microservices are fully decoupled so each service can be deployed on its own schedule without impacting the others."),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"Releasing microservices",src:i(1835).Z,width:"957",height:"406"})),(0,a.kt)("p",null,"The ability to independently deploy and release microservices is a chief benefit of microservice architectures. However, it introduces the problem of needing to manage multiple pipelines."),(0,a.kt)("h2",{id:"6-cicd-challenges-with-microservices"},"6. CI/CD Challenges with Microservices"),(0,a.kt)("p",null,"Implementing a CI/CD pipeline can differ quite a bit for monoliths and microservices. Let\u2019s explore these differences and the specific challenges faced by microservice-oriented CI/CD pipelines."),(0,a.kt)("h3",{id:"61-pipeline-management-challenges"},"6.1 Pipeline Management Challenges"),(0,a.kt)("h4",{id:"the-many-pipeline-problem"},"The Many-Pipeline Problem"),(0,a.kt)("p",null,"One approach to fully decoupling microservice deployments is to attach an individual CI/CD pipeline to each service. Since microservice teams are usually autonomous, it is common for teams to build their own pipelines. A benefit of this approach is that teams have full control and can customize all pipeline steps."),(0,a.kt)("p",null,"However, this many-pipeline approach adds complexity. Internal teams must now maintain numerous pipelines and their associated YAML files, scripts, and software versions. For example, when Expedia experienced an \u201cexplosion in the number of CI/CD pipelines\u201d, the engineering teams found that they were \u201cconstantly needing to update\u201d the pipelines for each microservice."),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"Many pipelines",src:i(5891).Z,width:"747",height:"442"})),(0,a.kt)("p",null,"The problem becomes worse as microservice architectures scale. With so many pipelines in the system, teams often have a hard time keeping up with how to build, test, and deploy each microservice. This could make it challenging for them to make system-wide adjustments quickly, for example rolling back a buggy microservice that has broken the production system."),(0,a.kt)("p",null,"In order to ease the burden of managing CI/CD for tens or potentially hundreds of microservices, modularization techniques have emerged in the CI/CD space."),(0,a.kt)("h4",{id:"the-shared-segment-solution"},"The Shared Segment Solution"),(0,a.kt)("p",null,"One solution for modularizing CI/CD pipelines across microservices is to reuse the same sequence of steps, known as segments, for different microservice pipelines. These shared segments could come in the form of shell scripts, reusable Docker images, repositories or libraries, or YAML templates containing deployment-related logic."),(0,a.kt)("p",null,"However, there are some major downsides to this approach. For one, it still requires bootstrapping and maintaining an individual pipeline for each microservice. Secondly, the shared pipeline segments themselves need to be maintained."),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"Shared segments",src:i(5378).Z,width:"560",height:"606"})),(0,a.kt)("h4",{id:"the-single-parameterized-pipeline-solution"},"The Single, Parameterized Pipeline Solution"),(0,a.kt)("p",null,"In the many-pipeline approach, each microservice has its own dedicated CI/CD pipeline. However, these pipelines can be made more flexible and reusable by parameterizing them. For instance, instead of linking a pipeline to a single repository URL, testing command, and entry points to configuration files, these values can be configurable for each service. This means that adding an additional microservice to the pipeline is simply a matter of configuring these parameters."),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"Shared pipeline",src:i(2386).Z,width:"807",height:"356"})),(0,a.kt)("p",null,"While this single-pipeline approach can offer significant benefits in terms of efficiency and consistency, it may not be the best fit for every team. To make it work, there must be a certain degree of uniformity across microservices in terms of how they are compiled, packaged, tested, and deployed. For microservices with more heterogeneous deployment requirements, a different approach may be needed."),(0,a.kt)("h3",{id:"62-microservice-testing-challenges"},"6.2 Microservice Testing Challenges"),(0,a.kt)("p",null,"As a reminder, a monolithic application runs in its own process, meaning components need not communicate over the network to interact. Running tests that verify components or modules in an application interface properly together is merely a matter of making function calls within that process."),(0,a.kt)("p",null,"On the contrary, microservices each run in their own process and are distributed across the network. It follows that testing microservice interactions also requires making network calls."),(0,a.kt)("h4",{id:"solutions-for-testing-service-interactions"},"Solutions for Testing Service Interactions"),(0,a.kt)("p",null,"There are countless possible setups for testing the interactions between microservices, but some of the most common are:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Test the service against a service spun up only for testing."),(0,a.kt)("li",{parentName:"ul"},"Test the service against a production instance of another service."),(0,a.kt)("li",{parentName:"ul"},"Run an entire pre-production environment to test the system as a whole.")),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"Microservice testing",src:i(4039).Z,width:"933",height:"560"})),(0,a.kt)("p",null,"There are advantages and disadvantages to each of these techniques, so a comprehensive CI/CD pipeline testing strategy will usually employ a combination of them."),(0,a.kt)("p",null,"Now that we\u2019ve looked into what a CI/CD pipeline for microservices looks like, let\u2019s walk through steps a development team might need to take to implement a CI/CD pipeline for microservices on their own."),(0,a.kt)("h2",{id:"7-manually-building-a-cicd-pipeline-for-microservices"},"7. Manually Building a CI/CD Pipeline for Microservices"),(0,a.kt)("p",null,"Building a CI/CD pipeline from scratch can be a time-consuming and difficult endeavor, especially if we are designing it to handle the inherent complexities of a microservices architecture. Smaller teams with limited experience with cloud infrastructure and automation may struggle to architect a robust pipeline. They might also lack the staff and expertise to maintain and optimize it. The following is an example list of tasks for setting up a pipeline on AWS."),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"Building a pipeline",src:i(4278).Z,width:"1073",height:"803"})),(0,a.kt)("p",null,"Instead of investing significant time and effort into this DIY project, teams may choose to leverage existing CI/CD solutions to simplify the process."),(0,a.kt)("h2",{id:"8-existing-solutions"},"8. Existing Solutions"),(0,a.kt)("h3",{id:"81-diy-solutions"},"8.1 DIY Solutions"),(0,a.kt)("p",null,"There are many free open-source DIY CI/CD tools such as Jenkins and Tekton. These tools offer a high level of customization and configurability, which allows the tool to be tailored for close alignment to a specific use case."),(0,a.kt)("p",null,"Jenkins achieves this level of customization through integration with numerous available plugins. This allows users to add functionality to Jenkins and tailor it to their needs. These tools also allow for pipeline modularization and reusability. For example, Jenkins accomplishes pipeline modularization through shared libraries while Tekton allows for the reusability of different subcomponents, such as tasks and pipelines."),(0,a.kt)("p",null,"However, these tools also come with the disadvantage of having a steep learning curve due to their high configurability, which makes the setup procedure more complex. For Jenkins, users need to have knowledge of relevant plugins that are needed to achieve the desired functionality. For Tekton, experience with Kubernetes is required since it runs as an extension on a Kubernetes cluster."),(0,a.kt)("h3",{id:"82-commercial-solutions"},"8.2 Commercial Solutions"),(0,a.kt)("p",null,"There are various commercial CI/CD pipelines available such as Codefresh, Semaphore, CircleCI, and AWS CodePipeline. Like the open-source tools, these solutions usually provide a high degree of customizability. For example, a YAML file might be used to configure the pipeline and fully customize the stages and their sequence."),(0,a.kt)("p",null,"Many commercial CI/CD solutions offer pipeline modularization and reusability. With CodeFresh, a single pipeline can be linked to multiple repositories, and the associated environment variables can be passed to the pipeline stages for use. Semaphore supports a monorepo approach, in which multiple applications, stored in the same repository, can each be linked to a variation of the same CI/CD pipeline."),(0,a.kt)("p",null,"Some commercial CI/CD solutions make testing microservice interactions easier. For example, Codefresh allows the user to specify \u201csidecar containers\u201d as part of pipelines, using Docker Compose under the hood to provision an environment to run integration tests."),(0,a.kt)("p",null,"However, commercial solutions may not be suitable for all scenarios. They are not as extensible as open-source solutions and may lack sensible default settings, despite being generally easier to use."),(0,a.kt)("h3",{id:"83-a-solution-for-our-use-case"},"8.3 A Solution for Our Use Case"),(0,a.kt)("p",null,"We wanted to build a solution for a specific use case. The solution would be open source as well as fully self-hosted on a user\u2019s AWS infrastructure allowing for complete control of code and data ownership. Next, the solution should be easy for users to set up and immediately integrate into their existing infrastructure. Here, the goal is to have low-configuration as well as sensible default settings that can meet the typical demands placed on a CI/CD pipeline. The solution should make managing deployment of multiple microservices easy through the use of a single, reusable, pipeline for every service. Lastly, the solution should provide options for testing microservice interactions."),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"Comparison chart",src:i(748).Z,width:"868",height:"392"})),(0,a.kt)("h2",{id:"9-introducing-seamless"},"9. Introducing Seamless"),(0,a.kt)("p",null,"Seamless is an open-source CI/CD pipeline tool designed specifically for containerized microservices deployed to AWS Elastic Container Service (ECS) Fargate. It offers a user-friendly interface that is similar to many of the popular interfaces found in commercial solutions. Unlike other CI/CD pipelines, Seamless does not require user-defined scripting through a YAML file template for configuration. Instead, Seamless relies on a core set of default stages: Prepare, Code Quality, Unit Test, Build, Integration Test, Deploy to Staging, and Deploy to Production. This approach makes Seamless easy to use right out of the box. Through the interface, users can simply provide the necessary commands needed to execute each stage, allowing for a more intuitive configuration."),(0,a.kt)("h3",{id:"91-setting-up-seamless"},"9.1 Setting Up Seamless"),(0,a.kt)("p",null,"Using Seamless requires:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"An AWS account"),(0,a.kt)("li",{parentName:"ul"},"npm installed"),(0,a.kt)("li",{parentName:"ul"},"The AWS CLI installed and configured (Seamless will use the locally configured AWS account)"),(0,a.kt)("li",{parentName:"ul"},"The AWS CDK command line tool installed")),(0,a.kt)("p",null,"To install Seamless, run ",(0,a.kt)("inlineCode",{parentName:"p"},"npm install -g seamless"),". Running ",(0,a.kt)("inlineCode",{parentName:"p"},"seamless init")," will guide the user through a series of inputs needed to deploy Seamless. After completing the initialization process, executing ",(0,a.kt)("inlineCode",{parentName:"p"},"seamless deploy")," will provision Seamless's infrastructure on AWS and provide a URL to access the platform's dashboard. The user can then navigate to the dashboard and start using Seamless."),(0,a.kt)("h3",{id:"92-using-seamless"},"9.2 Using Seamless"),(0,a.kt)("p",null,"The dashboard provides users with the ability to set up their pipeline and manage the associated services. When setting up the pipeline, the user will provide their ECS Cluster information for both production and staging environments. This allows for streamlined management of the pipeline and its services, providing users with a centralized location to manage all aspects of their deployment process."),(0,a.kt)("h3",{id:"connecting-services-to-the-pipeline"},"Connecting Services to the Pipeline"),(0,a.kt)("p",null,"The dashboard provides users with the ability to set up their pipeline and manage the associated services. When setting up the pipeline, the user will provide their ECS Cluster information for both production and staging environments."),(0,a.kt)("p",null,"After setting up the pipeline, the user can proceed to create services that will utilize the pipeline. The service setup process collects all the necessary information to run the pipeline, verify code functionality, and promote it to production. This accessible interface replaces a YAML file (or equivalent configuration) in many current solutions."),(0,a.kt)("p",null,(0,a.kt)("strong",{parentName:"p"},"(GIF of service setup form)")),(0,a.kt)("h3",{id:"running-the-pipeline"},"Running the Pipeline"),(0,a.kt)("p",null,"To activate the pipeline, the user can select from several triggers, including Push on Main, Open Pull Request, and Synchronize Pull Request. When Push on Main is triggered, it executes the full pipeline, while the Pull Request Options trigger a partial pipeline execution. Additionally, there is an option to manually execute the through a rerun feature included on the services page. Each service can be individually rerun with a button click."),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"Pipeline triggers",src:i(3106).Z,width:"1601",height:"864"})),(0,a.kt)("h3",{id:"monitoring-the-pipeline"},"Monitoring the Pipeline"),(0,a.kt)("p",null,"Seamless provides real-time monitoring of pipeline execution. The UI displays live updates of both runs and stages, enabling users to stay informed of pipeline outcomes as runs and stages transition from \u201cIdle\u201d to \u201cIn Progress\u201d, and ultimately to \u201cSuccess\u201d or \u201cFailure\u201d. Log data for each stage is updated live, making it easier to identify and troubleshoot errors when they occur."),(0,a.kt)("p",null,(0,a.kt)("strong",{parentName:"p"},"(GIF of live status updates)")),(0,a.kt)("h2",{id:"10-seamlesss-architecture"},"10. Seamless\u2019s Architecture"),(0,a.kt)("p",null,"To take advantage of the scalability and flexibility of cloud computing, we built Seamless\u2019s infrastructure on AWS using the Cloud Development Kit (CDK)."),(0,a.kt)("p",null,"When source code is updated, GitHub sends a webhook to start the pipeline. An Express.js backend running in an ECS Fargate cluster receives requests through an HTTP API Gateway. For each run, it retrieves pipeline data information from the PostgreSQL database (Relational Database Service \u2014 RDS) and sends it to the state machine (Step Function). The state machine executes each pipeline task in a container in ECS (EC2 launch type). All task containers mount a shared Docker volume on an Elastic File System (EFS) and can access the Elastic Container Registry for pushing or pulling required images. The updated source code is deployed to staging and production Fargate clusters."),(0,a.kt)("p",null,"During the pipeline run, the state machine sends status updates to the backend for storage in the database, and to users via the notification service. The task containers send logs to the backend to be inserted into the log cache (ElastiCache Redis). The backend sends both status updates and logs to the frontend dashboard via a WebSockets connection maintained by an API Gateway."),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"Architecture",src:i(3724).Z,width:"1358",height:"903"})),(0,a.kt)("h2",{id:"11-building-the-core-pipeline-functionality"},"11. Building the Core Pipeline Functionality"),(0,a.kt)("h3",{id:"111-modeling-and-storing-data"},"11.1 Modeling and Storing Data"),(0,a.kt)("p",null,"Our data model comprises four fundamental entities: Pipelines, Services, Runs, and Stages. To embrace our single-pipeline, many services approach, we aimed to ensure our data model reflected this by creating a one-to-many relationship between Pipelines and Services. Additionally, each Service can have many Runs, and each Run can have many Stages."),(0,a.kt)("p",null,"Given our schema's fixed nature, we chose to store our data in PostgreSQL, a relational database. We rely on the Prisma ORM to simplify schema creation and migration, as well as data manipulation."),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"ERD",src:i(2779).Z,width:"1674",height:"467"})),(0,a.kt)("p",null,"To accommodate the potentially high write speeds required to store logs generated by task containers, we sought a storage mechanism capable of supporting this. As a result, we decided to use a Redis cache (specifically, AWS ElastiCache) to meet our needs."),(0,a.kt)("h3",{id:"112-managing-pipeline-execution"},"11.2 Managing Pipeline Execution"),(0,a.kt)("p",null,"Pipelines execute tasks across multiple services, and we wanted a central orchestrator to manage the execution flow. We handled this using a state machine, which is a model for representing system behavior. It enabled us to define all potential states and the events that would trigger a transition from one state to another. One drawback is that defining states in advance can limit the ability to add new steps dynamically. As a result, the logic of the pipeline would remain unchanged from the beginning. We determined that this tradeoff was acceptable for our use case."),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"State machine example",src:i(1452).Z,width:"998",height:"390"})),(0,a.kt)("p",null,"We chose AWS Step Functions (state machine service) for its integration with other AWS services, flow visualizations, and built-in functionality for error handling. Before and after each state, we added steps to report system status to our backend. We also evaluated the XState JavaScript library, but it lacked native AWS integrations and proved more complex to scale and persist state."),(0,a.kt)("p",null,"We considered job queues as an alternative to state machines, but decided against it due to the difficulty in passing data between jobs and ensuring only one stage executed at a time. Another alternative was an event-driven architecture where each task would call the next, and there would be no orchestrator. However, we felt that having a central place to manage state made it easier to understand and debug."),(0,a.kt)("h3",{id:"113-running-tasks"},"11.3 Running Tasks"),(0,a.kt)("p",null,"For the actual tasks themselves, we use containers on ECS/EC2, Amazon\u2019s Elastic Container Service. Containers provide consistent environments, so multiple pipeline runs will behave the same way. They are lightweight, containing only necessary dependencies, and are ephemeral and elastic, meaning that ECS spins them up and down automatically to match demand. This aligns with our use case as pipeline tasks are ephemeral, and their demand varies depending on the timing of the pipeline runs. However, there are some downsides including cold start delays, management of the underlying EC2 instances, and needing to configure a shared persistent data store."),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"Task containers",src:i(1255).Z,width:"801",height:"655"})),(0,a.kt)("p",null,"We looked into alternative approaches, such as running containers on ECS Fargate, which would reduce management overhead, but it was unsuitable due to our need for Docker-in-Docker functionality. This is required for tasks such as building services as images, where Docker needs to run within our task runners. Another option was to run task logic directly on virtual machines, which would be easier to implement but would increase the management overhead and be less resource-efficient than using containers. It would also require configuring Amazon Machine Images and configuration management tools. Ultimately, we determined that using containers on ECS/EC2 was the most effective solution for our needs."),(0,a.kt)("h3",{id:"114-automating-pipeline-runs"},"11.4 Automating Pipeline Runs"),(0,a.kt)("p",null,"The pipeline runs automatically in response to changes in source code, and no manual intervention is required. During setup, the user chooses what triggers a run: Push to Main, Pull Request Open, and/or Pull Request Synchronize. Seamless creates a webhook in the repository. The backend uses the webhook payload to identify the trigger and initiate the appropriate pipeline process (partial or full run)."),(0,a.kt)("h2",{id:"12-improving-core-functionality"},"12. Improving Core Functionality"),(0,a.kt)("p",null,"Once Seamless\u2019s core functionality was working, we added features and optimizations to improve pipeline performance and user experience."),(0,a.kt)("h3",{id:"121-sharing-data-among-containers"},"12.1 Sharing Data Among Containers"),(0,a.kt)("p",null,"To minimize repeated work, we needed to ensure that multiple pipeline tasks could access the same files. For example, the Prepare Stage clones the source code so the Build Stage can package it into a Docker image later. To achieve this, we used the AWS EFS network file system, which is designed to be mounted to any number of EC2 instances or ECS containers. EFS is elastic and provides the necessary storage without needing to specify capacity in advance."),(0,a.kt)("p",null,"When each container is started, it is automatically mounted to a shared persistent Docker volume on EFS. The git commit hash serves as the directory name for the source code, which prevents naming conflicts and enables parallel pipeline execution. We also considered block storage, which had higher performance, but it was not suitable because it was not designed to connect to multiple containers."),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"NFS",src:i(503).Z,width:"530",height:"372"})),(0,a.kt)("h3",{id:"122-running-docker-commands-inside-docker-containers"},"12.2 Running Docker Commands Inside Docker Containers"),(0,a.kt)("p",null,"Some Task Containers run Docker commands inside. The Build Stage runs ",(0,a.kt)("inlineCode",{parentName:"p"},"docker build"),", and the Integration Test Stage runs ",(0,a.kt)("inlineCode",{parentName:"p"},"docker compose up"),". To enable this, we considered the \"Docker-in-Docker\" (DinD) approach, where a Docker container runs another Docker daemon inside it. However, we chose not to use DinD due to security, storage, and build cache issues. Instead, we opted to bind-mount the container's Docker socket with the host machine's Docker socket, which grants access to the host's Docker daemon. Although this doesn't create a nested \u201cparent-child\u201d Docker environment, it allows us to run \"sibling\" containers at the top level and share the host's build cache when starting a container. Since we were only running temporary jobs that didn't require a nested environment, this approach was sufficient for our needs."),(0,a.kt)("h3",{id:"123-integration-testing"},"12.3 Integration Testing"),(0,a.kt)("p",null,"Integration testing involves spinning up multiple containerized services to evaluate inter-service communication and functionality. To facilitate container management and networking, we used Docker Compose. This minimizes potential network faults and latency issues when testing service interactions. It also parallels the user's existing workflow if they are already using Docker Compose for local testing. We chose not to use a dedicated Fargate Cluster because it would have been slower and required additional infrastructure."),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"Integration test",src:i(1096).Z,width:"871",height:"693"})),(0,a.kt)("h3",{id:"124-manual-approval-of-staging-environments"},"12.4 Manual Approval of Staging Environments"),(0,a.kt)("p",null,"When integrating the state machine with other services, we had two primary patterns to choose from: Synchronous Jobs and Wait for a Callback Token. The synchronous model was suitable for most stages because each stage should automatically start after the previous one finishes. However, if the user disables Auto-Deploy, the state machine should pause so the developer can perform quality checks on the staging environment. This second scenario was a good use case for the Wait for a Callback Token pattern."),(0,a.kt)("h3",{id:"125-rollbacks"},"12.5 Rollbacks"),(0,a.kt)("p",null,"Rollbacks are a critical feature because they allow teams to revert to a previous stable version in case of unexpected issues. To enable rollbacks, we tag all Docker images with the git commit hash. Our UI displays all possible rollback images, giving users a choice of rollback targets. Each service can be rolled back independently, minimizing the impact on the overall deployment."),(0,a.kt)("h3",{id:"126-automatic-deployment-of-fargate-clusters"},"12.6 Automatic Deployment of Fargate Clusters"),(0,a.kt)("p",null,"To streamline the deployment process, we built a CDK feature that automatically deploys their Docker images to a Fargate Cluster and implements service discovery using AWS Service Connect. This approach helps users become productive quickly, as they only need to provide basic information about their service and its image. The feature can be used to set up both staging and production environments."),(0,a.kt)("h2",{id:"13-beyond-the-core-pipeline"},"13. Beyond the Core Pipeline"),(0,a.kt)("h3",{id:"131-designing-for-performance-and-scale"},"13.1 Designing for Performance and Scale"),(0,a.kt)("h4",{id:"serverless-backend"},"Serverless Backend"),(0,a.kt)("p",null,"Seamless\u2019s backend is a containerized Express.js application running on Fargate, which will spin up as many containers as needed in response to demand. This ensures that it can handle a large number of incoming requests without sacrificing performance."),(0,a.kt)("h4",{id:"parallel-execution-of-state-machines"},"Parallel Execution of State Machines"),(0,a.kt)("p",null,"Seamless enables parallel execution of state machines by utilizing separate instances of AWS Step Functions. This allows for concurrent execution, enabling different microservices to use the shared pipeline simultaneously. As a result, queuing pipeline executions becomes unnecessary and multiple microservices can be deployed at the same time."),(0,a.kt)("h3",{id:"132-notifications"},"13.2 Notifications"),(0,a.kt)("p",null,"Seamless offers integration with AWS Simple Notification Service (SNS), allowing for notifications to be sent via email, Slack, and PagerDuty. This feature provides added convenience and flexibility for users, ensuring that they can stay up-to-date with pipeline execution and quickly address any issues that arise."),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"Notifications",src:i(6338).Z,width:"842",height:"646"})),(0,a.kt)("h3",{id:"133-logging"},"13.3 Logging"),(0,a.kt)("p",null,"We developed a system to capture logs from all task containers, storing them in Redis with a 48-hour expiration time. It allows customization of the log payload and labels them using a ULID. We used sorted sets to insert logs in sorted order, eliminating the need for sorting when reading logs. The Dashboard shows users all logs for each stage, and any new logs are sent over WebSockets."),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"Logging",src:i(2102).Z,width:"866",height:"373"})),(0,a.kt)("h3",{id:"134-basic-security"},"13.4 Basic Security"),(0,a.kt)("h4",{id:"oauth"},"OAuth"),(0,a.kt)("p",null,"Seamless utilizes the OAuth flow, using Github\u2019s OAuth implementation to authenticate users and provide authorization to Github. When a user logs in, they are directed to Github, at which point a temporary code is generated for the user. The user then passes that code to an authorization proxy on Seamless\u2019s backend on subsequent requests. This token is used for a few purposes: to verify that the user is authenticated when they try to access any route on Seamless\u2019s backend, to configure webhooks on the user\u2019s behalf, and to clone the users\u2019 repositories."),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"OAuth flow",src:i(4875).Z,width:"1559",height:"677"})),(0,a.kt)("h4",{id:"private-subnets"},"Private Subnets"),(0,a.kt)("p",null,"Aside from our public-facing API gateway, most AWS resources in Seamless\u2019s infrastructure are provisioned in private subnets so they can\u2019t accept incoming network traffic. In case a developer needs to interact with resources in private subnets, such as their relational database or Redis cache, we deploy a bastion host that a developer can SSH into."),(0,a.kt)("h2",{id:"14-future-work"},"14. Future Work"),(0,a.kt)("p",null,"Seamless could be improved to support more use cases and offer more functionality. These features include:\nExpand deployment options beyond ECS Fargate\nSupport microservices not built using a Node.js runtime environment."),(0,a.kt)("h2",{id:"references"},"References"),(0,a.kt)("p",null,(0,a.kt)("sup",{parentName:"p",id:"fnref-1"},(0,a.kt)("a",{parentName:"sup",href:"#fn-1",className:"footnote-ref"},"1"))," ",(0,a.kt)("a",{parentName:"p",href:"https://www.cmswire.com/information-management/version-control-systems-the-link-between-development-and-deployment/"},"https://www.cmswire.com/information-management/version-control-systems-the-link-between-development-and-deployment/")))}p.isMDXComponent=!0},3724:(e,t,i)=>{i.d(t,{Z:()=>n});const n=i.p+"assets/images/architecture-simplified-606d977b67bdd8ce20e460ad2a7feb66.svg"},9449:(e,t,i)=>{i.d(t,{Z:()=>n});const n=i.p+"assets/images/auto-merge-bd6beb94cf6047043060205fc130df76.svg"},2708:(e,t,i)=>{i.d(t,{Z:()=>n});const n=i.p+"assets/images/branching-strategies-da54dc7b964fab0fbe0bbdc0cceffcde.svg"},4278:(e,t,i)=>{i.d(t,{Z:()=>n});const n=i.p+"assets/images/building-a-pipeline-c6c5d098d19ae5716ddcfe8b8bb66b53.svg"},4437:(e,t,i)=>{i.d(t,{Z:()=>n});const n=i.p+"assets/images/cicd-simplified-0c327a319f6641d1b60ec82f5a89ccd5.svg"},748:(e,t,i)=>{i.d(t,{Z:()=>n});const n=i.p+"assets/images/comparison-chart-0e51cb8eb59d227702fa47c04e6fe331.svg"},1096:(e,t,i)=>{i.d(t,{Z:()=>n});const n=i.p+"assets/images/integration-test-c6006f8344e1172555bfdb897a60a4de.svg"},2102:(e,t,i)=>{i.d(t,{Z:()=>n});const n=i.p+"assets/images/logs-c9bfd52ab488bae845f48b55ac801900.svg"},1784:(e,t,i)=>{i.d(t,{Z:()=>n});const n=i.p+"assets/images/manual-deployment-delays-894477d3f161f9e3e7a418f32fdf367d.svg"},6197:(e,t,i)=>{i.d(t,{Z:()=>n});const n=i.p+"assets/images/manual-vs-automatic-deployment-e1166efdcdc6bdcfebf66ff36772e065.svg"},5891:(e,t,i)=>{i.d(t,{Z:()=>n});const n=i.p+"assets/images/many-pipelines-697cfd4597b842b30c367c5cc639140a.svg"},4039:(e,t,i)=>{i.d(t,{Z:()=>n});const n=i.p+"assets/images/microservice-testing-0fb3a057e65e9af7d5dd20ef129cf177.svg"},1554:(e,t,i)=>{i.d(t,{Z:()=>n});const n=i.p+"assets/images/monolith-microservices-b11e7d5118f329a5f5f797ef242ebb02.svg"},503:(e,t,i)=>{i.d(t,{Z:()=>n});const n=i.p+"assets/images/nfs-6bee67c83988e313985365803aec6f72.svg"},4875:(e,t,i)=>{i.d(t,{Z:()=>n});const n=i.p+"assets/images/oauth-flow-c87fd2b83801538f2ff04372898a0bb0.svg"},2779:(e,t,i)=>{i.d(t,{Z:()=>n});const n=i.p+"assets/images/one-to-many-79c87028e654b1d92846102c79c5a3b6.svg"},3106:(e,t,i)=>{i.d(t,{Z:()=>n});const n=i.p+"assets/images/pipeline-triggers-0b27df8af4d8b8596db3258cc0fed9e1.svg"},1835:(e,t,i)=>{i.d(t,{Z:()=>n});const n=i.p+"assets/images/releasing-microservices-8ebf562d256cd7d434183d96708e0eb9.svg"},428:(e,t,i)=>{i.d(t,{Z:()=>n});const n=i.p+"assets/images/same-app-different-results-958bca494ed7fe7e2b922f19b3902c02.svg"},2386:(e,t,i)=>{i.d(t,{Z:()=>n});const n=i.p+"assets/images/shared-pipeline-2dc4889e8821c967d3e5fb363ff2675e.svg"},5378:(e,t,i)=>{i.d(t,{Z:()=>n});const n=i.p+"assets/images/shared-segments-2cc3c9d0eb04584e8a743cdcce18667e.svg"},9339:(e,t,i)=>{i.d(t,{Z:()=>n});const n=i.p+"assets/images/stages-20e1f509374cdf4c8da9345c75833898.svg"},1452:(e,t,i)=>{i.d(t,{Z:()=>n});const n=i.p+"assets/images/state-machine-example-b9beada3e80c508fe2d0383179c4b6af.svg"},6338:(e,t,i)=>{i.d(t,{Z:()=>n});const n=i.p+"assets/images/status-updates-notifications-0b423418844aaa373dc44734f77c820d.svg"},1255:(e,t,i)=>{i.d(t,{Z:()=>n});const n=i.p+"assets/images/task-containers-74dd7176926b8794b8de9f10aace7032.svg"}}]);